---
title: "IDS Final Project"
author: "Bilwa Wagh, Gina O'Riordan, Mitchell Speer, Sharif Nijim"
date: "09/17/2018"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# initialize the environment
rm(list=ls()) 
# load the libraries and data
#setwd("~/Documents/GitHub/SL-Fall-2018-Final-Project") #Bilwa
setwd("C:/Users/Owner/Desktop/Data Science/Stat Learning/Datasets/Stat-Learning-2018-Fall-master/Classification_Final_Data/Absenteeism_at_work_AAA") #Mitch
#setwd("/Volumes/GoogleDrive/My Drive/MSDS/04.SL/SL.Team.Project") # Sharif
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tools, caret, knitr, tidyverse, kableExtra, rpart, rpart.plot, corrplot,
               MASS, tree, randomForest, obliqueRF, partykit, rminer, ROCR, pROC,
               mlbench, class, reshape2, stats, locfit, dmm)
options(warn=-1)
#library(neuralnet)
```
 
# Section 1: Prepare the Data
```{r}
#absentee <- read.csv("C:/Users/Owner/Desktop/Data Science/Stat Learning/Datasets/Stat-Learning-2018-Fall-master/Classification_Final_Data/Absenteeism_at_work_AAA/Absenteeism_at_work.csv", sep = ';')
# The Absenteeism dataset is a semi-colon separated CSV
absentee <- read.csv("C:/Users/Owner/Desktop/Data Science/Stat Learning/Datasets/Stat-Learning-2018-Fall-master/Classification_Final_Data/Absenteeism_at_work_AAA/Absenteeism_at_work.csv", sep = ';')
# Now that we have loaded the data, take a brief look at it
glimpse(absentee)
```
  
# Section 2: Exploratory Data Analysis
In this section, we will perform an initial inspection of the absenteeism data.  We will examine data quality and the shape of the data.
  
## Check data quality
```{r}
# check for missing data
sum(is.na(absentee))
```
Based on the above, the data is clean.  There are no missing values, so it is not necessary to transform or cleanse the data before working on it.  
  
As part of the EDA, perform an analysis of each variable to understand the missingness in each attribute, variance, and shape.
  
## Explore Reason for Absence
```{r}
# Explore Reason for Absence
table(absentee$Reason.for.absence)
```
  
```{r}
# Explore Reason for Absence
hist(absentee$Reason.for.absence)
```
## Explore Month of absence
```{r}
# Explore Month of absence
table(absentee$Month.of.absence)
```
  
## Explore Reason for Absence
```{r}
# Explore Reason for Absence
hist(absentee$Month.of.absence)
```
## Explore Day of the week
```{r}
# Explore Day of the week
table(absentee$Day.of.the.week)
```
## Explore Day of the week
```{r}
# Explore Day of the week
hist(absentee$Day.of.the.week)
```
## Explore seasons
```{r}
# Explore seasons
table(absentee$Seasons)
```
```{r}
# Explore seasons
hist(absentee$Seasons)
```
## Explore transportation expense
```{r}
# Explore Transportation expense
table(absentee$Transportation.expense)
```
```{r}
# Explore Transportation expense
hist(absentee$Transportation.expense)
```
## Explore distance from residence to work
```{r}
# Explore Distance from residence
table(absentee$Distance.from.Residence.to.Work)
```
```{r}
# Explore Distance from residence
hist(absentee$Distance.from.Residence.to.Work)
```
## Explore service time
```{r}
# Explore Service time
table(absentee$Service.time)
```
```{r}
# Explore Service time
hist(absentee$Service.time)
```
## Explore age
```{r}
# Explore Age
table(absentee$Age)
```
```{r}
# Explore age
hist(absentee$Age)
```
## Explore work load average
```{r}
# Explore work load average
table(absentee$Work.load.Average.day)
```
```{r}
# Explore work load average
hist(absentee$Work.load.Average.day)
```
## Explore hit target
```{r}
# Explore hit target
table(absentee$Hit.target)
```
```{r}
# Explore hit target
hist(absentee$Hit.target)
```
## Explore disciplinary failure
```{r}
# Explore disciplinary failure
table(absentee$Disciplinary.failure)
```
```{r}
# Explore disciplinary failure
hist(absentee$Disciplinary.failure)
```
## Explore education
```{r}
# Explore Education
table(absentee$Education)
```
```{r}
# Explore Education
hist(absentee$Education)
```
## Explore number of children
```{r}
# Explore number of children
table(absentee$Son)
```
```{r}
# Explore number of children
hist(absentee$Son)
```
## Explore social drinker
```{r}
# Explore social drinker
table(absentee$Social.drinker)
```
```{r}
# Explore social drinker
hist(absentee$Social.drinker)
```
## Explore social smoker
```{r}
# Explore social smoker
table(absentee$Social.smoker)
```
```{r}
# Explore social smoker
hist(absentee$Social.smoker)
```
## Explore pet ownership
```{r}
# Explore pet ownership
table(absentee$Pet)
```
```{r}
# Explore pet ownership
hist(absentee$Pet)
```
## Explore weight
```{r}
# Explore weight
table(absentee$Weight)
```
```{r}
# Explore weight
hist(absentee$Weight)
```
## Explore height
```{r}
# Explore height
table(absentee$Height)
```
```{r}
# Explore height
hist(absentee$Height)
```
## Explore BMI
```{r}
# Explore BMI
table(absentee$Body.mass.index)
```
```{r}
# Explore BMI
hist(absentee$Body.mass.index)
```
## Explore Absenteeism time in hours
```{r}
# Explore Absenteeism time in hours
table(absentee$Absenteeism.time.in.hours)
```
```{r}
# Explore Absenteeism time in hours
hist(absentee$Absenteeism.time.in.hours)
```
```{r}
# Examine the relationship between reason for absence and absenteeism in hours
count(absentee %>%
  dplyr::filter(Reason.for.absence == 0) %>%
  dplyr::select(Reason.for.absence, Absenteeism.time.in.hours))
```
Whenever reason for absence is zero, the time in hours absent is also zero.  

## Check for zero variance
```{r}
dat <- absentee %>%
  mutate(new = 1)
nearZeroVar(dat)
```
## Look for correlations in the data
```{r}
corrplot::corrplot(cor(absentee),  tl.cex = .6)
```
Looking at the above plot, there is a high correlation between age and service time.  To refrain from potential discrimination issues, we will exclude age from the analysis.

Looking at the above, we see a correlation between BMI, weight, and height.  This makes intuitive sense, as BMI is a calculation based on weight and height.  Since BMI effectively represents the impact of both weight and height, both weight and height will be excluded from the analysis.  
  
```{r}
# See if there is a relationship between the reason for an absence
# and the amount of time absent
ggplot(absentee, aes(as.factor(Reason.for.absence), Absenteeism.time.in.hours, color = as.factor(Month.of.absence))) + geom_point()
```

```{r}
# For absences less than a week in duration, see if there is a relationship with
# the number of hours and the distance between home and work
absentee %>%
  filter(Absenteeism.time.in.hours <= 40) %>%
ggplot(aes( Absenteeism.time.in.hours, Distance.from.Residence.to.Work)) + geom_col()
```

```{r}
# See if there is a relationship between worker productivity and number of hours absent
ggplot(absentee, aes(as.factor(absentee$Hit.target), Absenteeism.time.in.hours)) + geom_boxplot()
```
```{r}
# Explore the relationship between service time and absenteeism.
ggplot(absentee, aes(absentee$Service.time, absentee$Absenteeism.time.in.hours)) + geom_point()
```
Based on the plot above, there is a trend to missing more time at work based on years of service.  
  
```{r}
# Check to see if in aggregate, there are months that have greater absenteeism than others
# Also check to see if any specific day stands out as being prone to absenteeism.
ggplot(absentee, aes(absentee$Month.of.absence, absentee$Absenteeism.time.in.hours, fill = as.factor(absentee$Day.of.the.week))) + geom_col()
```

## Explore data sparseness
```{r}
# explore sparseness
table(absentee$Disciplinary.failure)
# pretty sparse - get rid of it
```
```{r}
# explore sparseness
table(absentee$Social.smoker)
# pretty sparse - get rid of it
```
  
# Section 3:  Pre-processing
In the EDA, it became evident that age, weight, and height can be removed from the analysis due to correlation.  Disciplinary failure and smoker should be removed due to the sparseness of the data.  
```{r}
# create a new dataset without age, weight, height, Disciplinary Failure and Social Smoker
absentee.sparse <- absentee %>%
  dplyr::select(-Age, -Weight, -Height, -Disciplinary.failure, -Social.smoker)

# Categorize the absenteeism based on duration, using 16 hours based on it representing
# 2 consecutive days
absentee.work <- absentee.sparse %>%
  mutate(Absenteeism.time.in.hours = as.factor(ifelse(absentee$Absenteeism.time.in.hours < 16, "D","W")))
```
  
# Section 4:  Initial Models
In this section, we will explore regular trees, random forest, LDA, QDA, and SVM.

```{r}
set.seed(1842)
# training test split
id <- holdout(absentee.work$Absenteeism.time.in.hours,
              ratio=.6,
              mode='stratified')
  
absentee.tr <-  absentee.work[id$tr,]
absentee.te <-  absentee.work[id$ts,]
R <- 50
error_matrix <-  matrix(0, ncol=6, nrow=R)
sensitivity_matrix <-  matrix(0, ncol=6, nrow=R)
specificity_matrix <-   matrix(0, ncol=6, nrow=R)
  
for (r in 1:R) {
  # a. Run regular trees
  tree_mod <-  rpart(Absenteeism.time.in.hours ~ .,
                     data = absentee.tr)      
  
  # b. Run Random Forest (100 trees)  
  rf_reg <- randomForest(Absenteeism.time.in.hours ~ ., 
                        absentee.tr, 
                        ntree = 100)
  
  # c. Run LDA
  lda_absent <- lda(absentee.tr[,-16],
                   absentee.tr[,16])
  
  # d. Run QDA - Errors occurred on two different runs at various Reps with a "rank deficiency" alert.
  qda_absent <- qda(absentee.tr[,-16],
                   as.numeric(absentee.tr[,16]))
  
  # e. SVM (C)
  svmC_ab <- svm(absentee.tr[,-16],
                absentee.tr[,16],
                cross=5,
                C=0.25,
                type="C-classification")
  
  # f. SVM (Nu)
  svmNu_ab <- svm(absentee.tr$Absenteeism.time.in.hours~.,
                 data=absentee.tr,
                 cross=5,
                 nu=0.15,
                 type='nu-classification')
  
  
  ##Predict
  yhat_tree <- predict(tree_mod, absentee.te[,-16], type = 'class')
  yhat_rf <- predict(rf_reg, absentee.te[,-16])
  yhat_lda <- predict(lda_absent, absentee.te[,-16])$class
  yhat_qda <- predict(qda_absent, absentee.te[,-16])$class
  yhat_svmC <- predict(svmC_ab, absentee.te[,-16])
  yhat_svmNu <- predict(svmNu_ab, absentee.te[,-16])
  
  # store the errors in the matrix
  error_matrix[r,1] <- 1-mean(yhat_tree!=absentee.te[,16])
  error_matrix[r,2] <- 1-mean(yhat_rf!=absentee.te[,16])
  error_matrix[r,3] <- 1-mean(yhat_lda!=absentee.te[,16])
  error_matrix[r,4] <- 1-mean(as.numeric(yhat_qda)!=as.numeric(absentee.te[,16]))
  error_matrix[r,5] <- 1-mean(yhat_svmC!=absentee.te[,16])
  error_matrix[r,6] <- 1-mean(yhat_svmNu!=absentee.te[,16])
  
  cf <- confusionMatrix(yhat_tree,absentee.te[,16])
  sensitivity_matrix[r,1] <- cf$byClass[1]
  specificity_matrix[r,1] <- cf$byClass[2]
  
  cf <- confusionMatrix(yhat_rf,absentee.te[,16])
  sensitivity_matrix[r,2] <- cf$byClass[1]
  specificity_matrix[r,2] <- cf$byClass[2]

  cf <- confusionMatrix(yhat_lda, absentee.te[,16])
  sensitivity_matrix[r,3] <- cf$byClass[1]
  specificity_matrix[r,3] <- cf$byClass[2]
  
  cf <- confusionMatrix(as.factor(as.numeric(yhat_qda)),
                        as.factor(as.numeric(absentee.te[,16])))
  sensitivity_matrix[r,4] <- cf$byClass[1]
  specificity_matrix[r,4] <- cf$byClass[2]
  
  cf <- confusionMatrix(yhat_svmC,
                        absentee.te[,16])
  sensitivity_matrix[r,5] <- cf$byClass[1]
  specificity_matrix[r,5] <- cf$byClass[2]

  cf <- confusionMatrix(yhat_svmNu,
                        absentee.te[,16])
  sensitivity_matrix[r,6] <- cf$byClass[1]
  specificity_matrix[r,6] <- cf$byClass[2]
}
```

```{r}
# Label the columns
colnames(error_matrix) <- c('Trees',
                           'RF',
                           'LDA',
                           'QDA',
                           'SVM-C',
                           'SVM-Nu')
#melt the matrix
error_matrix_melt <- melt(as.data.frame(error_matrix))
#rename the columns
colnames(error_matrix_melt) <- c('Method','Accuracy')
#boxplot of the errors
ggplot(error_matrix_melt,mapping=aes(x=Method,y=Accuracy))+
            geom_boxplot()
```

```{r}
# Label the columns
colnames(sensitivity_matrix) <- c('Trees',
                                 'RF',
                                 'LDA',
                                 'QDA',
                                 'SVM-C',
                                 'SVM-Nu')
#melt the matrix
sensitivity_matrix_melt <- melt(as.data.frame(sensitivity_matrix))
#rename the columns
colnames(sensitivity_matrix_melt) <- c('Method','Sensitivity')
#boxplot of the errors
ggplot(sensitivity_matrix_melt,mapping=aes(x=Method,y=Sensitivity))+
            geom_boxplot()
```


```{r}
# Label the columns
colnames(specificity_matrix) <- c('Trees',
                                 'RF',
                                 'LDA',
                                 'QDA',
                                 'SVM-C',
                                 'SVM-Nu')
#melt the matrix
specificity_matrix_melt <- melt(as.data.frame(specificity_matrix))
#rename the columns
colnames(specificity_matrix_melt) <- c('Method','Specificity')
#boxplot of the errors
ggplot(specificity_matrix_melt,mapping=aes(x=Method,y=Specificity))+
            geom_boxplot()
```
  
# Section 5:  Optimize Models
Based on the initial exploration of the trees, random forest, LDA, QDA, and SVM, the SVM-Nu model consistently outperformed the others despite being in an unoptimized state.  As such, select SVM-Nu as the model of choice for this dataset, and proceed to optimize.  
  
```{r}
#50 replications and a 60%-training/40%-test data split to compute the average test error
set.seed(1842)
R <- 50 # set the number of replications
# create the error matrix to store values
error_matrix <-  matrix(0, ncol=2, nrow=R)
sensitivity_matrix <- matrix(0, ncol=2, nrow=R)
specificity_matrix <- matrix(0, ncol=2, nrow=R)

for (r in 1:R) {
  
  # training test split
  id <- holdout(absentee.work$Absenteeism.time.in.hours,
                ratio=.6,
                mode='stratified')
  
  absentee.tr <- absentee.work[id$tr,]
  absentee.te <- absentee.work[id$ts,]
  
  # a. SVM (C)
    
    # Number of C to observe   
    n.C <- 20 

        #  Create a sequence to try out 20 values between 0.01 and 0.99
        # note: 0 and 1 are not valid values to try   
        v.C <- seq(0.01,.99, length=n.C)
        cv.for.C <- numeric(n.C)
      
        for(j in 1:n.C) {
          
            # training test split (60-40 tr/te split)
             id <- holdout(absentee.tr$Absenteeism.time.in.hours,
               ratio=.6,
               mode='stratified')
  
             ab.tr <- absentee.tr[id$tr,]
             ab.te <- absentee.tr[id$ts,]
           
            # loop through each value of C to try
            C.svm.ab <- svm(Absenteeism.time.in.hours~.,
                           data=ab.tr,
                           cross=5,
                           C=v.C[j], 
                           type='C-classification') 
         
            # get the cross validation error for each C value 
            yhat_svm <- predict(C.svm.ab, ab.te[,-16])
            cv.for.C[j] <- mean(yhat_svm != ab.te[,16]) 
        }    
        
        # find the optimal C value
        v.opt.C <- v.C[min(which(cv.for.C==min(cv.for.C)))]
        
  svmC_ab <- svm(absentee.tr[,-16],
                absentee.tr[,16],
                cross=5,
                C=v.opt.C,
                type="C-classification")
  
  # b. SVM (Nu)
    
    # Number of nu to observe   
    n.nu <- 30 

        #  Create a sequence to try out 30 values between 0.01 and .1 
        # note: 0 and 1 are not valid values to try   
        v.nu <- seq(0.01,.1, length=n.nu)
        cv.for.nu <- numeric(n.nu)
      
        for(j in 1:n.nu) {
          
            # training test split (60-40 tr/te split)
             id <- holdout(absentee.tr$Absenteeism.time.in.hours,
               ratio=.6,
               mode='stratified')
  
             ab.tr <- absentee.tr[id$tr,]
             ab.te <- absentee.tr[id$ts,]
           
            # loop through each value of nu to try
            nu.svm.ab <- svm(Absenteeism.time.in.hours~.,
                            data=ab.tr,
                            cross=10,
                            nu=v.nu[j], 
                            type='nu-classification') 
         
            # get the cross validation error for each nu value 
            yhat_svm <- predict(nu.svm.ab, ab.te[,-16])
            cv.for.nu[j] <- mean(yhat_svm != ab.te[,16]) 
        }    
        
        # find the optimal nu value
        v.opt.nu <- v.nu[min(which(cv.for.nu==min(cv.for.nu)))]
        
  svmNu_ab <- svm(absentee.tr$Absenteeism.time.in.hours~.,
                 data=absentee.tr,
                 cross=10,
                 nu=v.opt.nu, 
                 type='nu-classification')
  
  # prediction    
  yhat_svmC <- predict(svmC_ab, absentee.te[,-16])
  yhat_svmNu <- predict(svmNu_ab, absentee.te[,-16])
  
  # store the errors in the matrix
  error_matrix[r,1] <- 1-mean(yhat_svmC!=absentee.te[,16])
  error_matrix[r,2] <- 1-mean(yhat_svmNu!=absentee.te[,16])
  
  cf <- confusionMatrix(yhat_svmC, absentee.te[,16])
  sensitivity_matrix[r,1] <- cf$byClass[1]
  specificity_matrix[r,1] <- cf$byClass[2]

  cf <- confusionMatrix(yhat_svmNu, absentee.te[,16])
  sensitivity_matrix[r,2] <- cf$byClass[1]
  specificity_matrix[r,2] <- cf$byClass[2]

  # just a nice statement to tell you when each loop is done
  cat("Finished Rep",r," and Nu: ",v.opt.nu," and C: ",v.opt.C, "\n")
}
```
  
```{r}
# Label the columns
colnames(error_matrix) <- c('SVM-C_OPT','SVM-Nu_OPT')
#melt the matrix
error_matrix_melt_opt <- melt(as.data.frame(error_matrix))
#rename the columns
colnames(error_matrix_melt_opt) <- c('Method','Accuracy')
#boxplot of the errors
ggplot(error_matrix_melt_opt,mapping=aes(x=Method,y=Accuracy))+
            geom_boxplot()
```



```{r}
# Label the columns
colnames(sensitivity_matrix) <- c('SVM-C_OPT','SVM-Nu_OPT')
#melt the matrix
sensitivity_matrix_melt_opt <- melt(as.data.frame(sensitivity_matrix))
#rename the columns
colnames(sensitivity_matrix_melt_opt) <- c('Method','Sensitivity')
#boxplot of the errors
ggplot(sensitivity_matrix_melt_opt,mapping=aes(x=Method,y=Sensitivity))+
            geom_boxplot()
```


```{r}
# Label the columns
colnames(specificity_matrix) <- c('SVM-C_OPT','SVM-Nu_OPT')
#melt the matrix
specificity_matrix_melt_opt <- melt(as.data.frame(specificity_matrix))
#rename the columns
colnames(specificity_matrix_melt_opt) <- c('Method','Specificity')
#boxplot of the errors
ggplot(specificity_matrix_melt_opt,mapping=aes(x=Method,y=Specificity))+
            geom_boxplot()
```

##How did the optimized models perform compared to the base models?
###Accuracy
```{r}
acc_init_opt <- rbind(filter(error_matrix_melt,error_matrix_melt$Method=="SVM-Nu"),filter(error_matrix_melt_opt,error_matrix_melt_opt$Method=="SVM-Nu_OPT"))

ggplot(acc_init_opt,mapping=aes(x=Method,y=Accuracy))+
            geom_boxplot()

error_matrix_melt_opt
```

###Sensitivity
```{r}
sens_init_opt <- rbind(filter(sensitivity_matrix_melt,sensitivity_matrix_melt$Method=="SVM-Nu"),filter(sensitivity_matrix_melt_opt,sensitivity_matrix_melt_opt$Method=="SVM-Nu_OPT"))

ggplot(sens_init_opt,mapping=aes(x=Method,y=Sensitivity))+
            geom_boxplot()

sensitivity_matrix_melt_opt
```

###Specificity
```{r}
spec_init_opt <- rbind(filter(specificity_matrix_melt,specificity_matrix_melt$Method=="SVM-Nu"),filter(specificity_matrix_melt_opt,specificity_matrix_melt_opt$Method=="SVM-Nu_OPT"))

ggplot(spec_init_opt,mapping=aes(x=Method,y=Specificity))+
            geom_boxplot()

specificity_matrix_melt_opt
```
Overall, we sacrificed a miniscule amount of sensitivity for the sake of predicting our absentee workers who are absent for more than two days.
